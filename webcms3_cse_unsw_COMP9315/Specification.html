<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <title>COMP9315 19T2 - Assignment 2</title>
  <link rel='stylesheet' type='text/css' href='Pics/course.css'>
</head>

<body>
  <div align='center'>
    <table width='100%' border='0'>
      <tr valign='top'>
        <td align='center' width='50%'>
          <span class='heading'>Assignment 2</span><br><b>Multi-attribute Linear Hashed Files</b>
        </td>
      </tr>
    </table>
  </div>
  <h2>Aims</h2>
  <p>
    This assignment aims to give you an understanding of
  </p>
  <ul>
    <li> how database files are structured and accessed
    <li> how multi-attribute hashing is implemented
    <li> how linear hashing is implemented
  </ul>
  <p>
    The goal is to build a simple implementation of a linear-hashed file
    structure that uses multi-attribute hashing.
  </p>


  <h2>Summary</h2>
  <p class="red">
    The <tt>ass2.tar</tt> file must contain your <tt>Makefile</tt> plus
    all of your <tt>*.c</tt> and <tt>*.h</tt> files. <br>
    Details on how to build the <tt>ass2.tar</tt> file are given below.
  </p>
  <p>
    Make sure that you read this assignment specification <em>carefully and
      completely</em> before starting work on the assignment.<br>
    Questions which indicate that you haven't done this will simply get the
    response "Please read the spec".
  </p>
  <p>
    <b>Note:</b> this assignment does not require you to do anything with PostgreSQL.
  </p>

  <h2>Introduction</h2>

  <p>
    Linear hashed files and multi-attribute hashing are two techniques that
    can be used together to produce hashed files that grow as needed and
    which allow all attributes to contribute to the hash value of each tuple.
    See the course notes and lecture slides for further details on linear
    hashed files and multi-attribute hashing.
  </p>
  <p>
    In our context, multi-attribute linear-hashed (MALH) files are file
    structures that represent one relational table, and can be manipulated
    by three commands:
  </p>
  <dl>
    <dt>A <b><tt>create</tt></b> command</dt>
    <dd>
      <p>
        Creates MALH files by accepting four command line arguments:
      </p>
      <ul>
        <li> the name of the relation
        <li> the number of attributes
        <li> the initial number of data pages (rounded up to nearest 2<sup>n</sup>)
        <li> the multi-attribute hashing choice vector
      </ul>
      <p>
        This gives you storage for one relation/table,
        and is analogous to making an SQL data definition like:
      </p>
      <pre>
create table R ( a<sub>1</sub> text, a<sub>2</sub> text, ... a<sub>n</sub> text );
</pre>
      <p>
        Note that, internally, attributes are indexed 0..<i>n</i>-1 rather
        than 1..<i>n</i>.
      </p>
      <p>
        The following example of using <tt>create</tt> makes a table called <tt>abc</tt>
        with 4 attributes and 8 initial data pages:
      </p>
      <pre>
$ <b>./create  abc  4  6  "0,0:0,1:1,0:1,1:2,0:3,0"</b>
</pre>
      <p>
        The choice vector (fourth argument above) indicates that
        <ul>
          <li> bit 0 from attribute 0 produces bit 0 of the MA hash value
          <li> bit 1 from attribute 0 produces bit 1 of the MA hash value
          <li> bit 0 from attribute 1 produces bit 2 of the MA hash value
          <li> bit 1 from attribute 1 produces bit 3 of the MA hash value
          <li> bit 0 from attribute 2 produces bit 4 of the MA hash value
          <li> bit 0 from attribute 3 produces bit 5 of the MA hash value
        </ul>
        <p>
          The following diagram illustrates this scenario:
        </p>
        <center><img src="Pics/chvec.png"></center>
        <p>
          The above choice vector only specifies 6 bits of the combined hash,
          but combined hashes contain 32 bits.
          The remaining 26 entries in the choice vector are automatically generated
          by cycling through the attributes and taking bits from the <i>high-order</i>
          hash bits from each of those attributes.
        </p>
    </dd>
    <dt>An <b><tt>insert</tt></b> command</dt>
    <dd>
      <p>
        Reads tuples, one per line, from standard input
        and inserts them into the relation specified on the command line.
        Tuples all take the form <i>val<sub>1</sub>,val<sub>2</sub>,...,val<sub>n</sub></i>.
        The values can be any sequence of characters except <tt>','</tt> and <tt>'?'</tt>.
      </p>
      <p>
        The bucket where the tuple is placed is determined by the appropriate number of
        bits of the combined hash value. If the relation has <i>2<sup>d</sup></i> data pages,
        then <i>d</i> bits are used. If the specified data page is full, then the tuple
        is inserted into an overflow page of that data page.
      </p>
    </dd>
    <dt>A <b><tt>select</tt></b> command</dt>
    <dd>
      </p>
      Takes a "query tuple" on the command line, and finds all tuples in
      either the data pages or overflow pages that match the query.
      Queries take the form <i>val<sub>1</sub>,val<sub>2</sub>,...,val<sub>n</sub></i>,
      where some of the <i>val<sub>i</sub></i> can be <tt>'?'</tt> (without the quotes).
      Such "attributes" represent wild-cards and can match any value in the
      corresponding attribute position.
      Some example query tuples, and their interpretation are given below.
      You can find more examples in the lecture slides and course notes.
      </p>
      <pre>
?,?,?    <span class="comment"># matches any tuple in the relation</span>
10,?,?   <span class="comment"># matches any tuple with 10 as the value of attribute 0</span>
?,abc,?  <span class="comment"># matches any tuple with abc as the value of attribute 1</span>
10,abc,? <span class="comment"># matches any tuple with 10 and abc as the values of attributes 0 and 1</span>
</pre>
    </dd>
  </dl>
  <p>
    A MALH relation <tt>R</tt> is represented by three physical files:
  </p>
  <ul>
    <li>
      <p>
        <tt>R.info</tt> containing global information such as
        <ul>
          <li> a count of the number of attributes
          <li> the depth of main data file (<i>d</i> for linear hashing)
          <li> the page index of the split pointer (<i>sp</i> for linear hashing)
          <li> a count of the number of main data pages
          <li> the total number of tuples (in both data and overflow pages)
          <li> the choice vector (<i>cv</i> for multi-attribute hashing)
        </ul>
      </p>
    </li>
    <li>
      <p>
        <tt>R.data</tt> containing data pages, where each data page contains
        <ul>
          <li> offset of start of free space
          <li> overflow page index (or <tt>NO_PAGE</tt> if none)
          <li> a count of the number of tuples in that page
          <li> the tuples (as comma-separated C strings)
        </ul>
      </p>
    </li>
    <li>
      <p>
        <tt>R.ovflow</tt> containing overflow pages, which have the same structure as data pages
      </p>
    </li>
  </ul>
  <p>
    When a MALH relation is first created, it is set to contain a <i>2<sup>n</sup></i>
    pages, with depth <i>d=n</i> and split pointer <i>sp=0</i>.
    The overflow file is initially empty.
    The following diagram shows an MALH file <tt>R</tt> with initial state with <i>n=2</i>.
  </p>
  <center><img src="Pics/lh1.png"></center>
  <p>
    After 294 tuples have been inserted, the file might have the following state
    (depending on field value distributions, tuple sizes, etc):
  </p>
  <center><img src="Pics/lh2.png"></center>
  <p>
    Pages in MALH files have the following structure:
    a header with three unsigned integers,
    strings for all of the tuple data,
    free space containing no tuple data.
    The following diagram gives an exmple of this:
  </p>
  <center><img src="Pics/pg.png"></center>
  <p>
    We have developed some infrastructure for you to use in implementing
    multi-attribute linear-hashed (MALH) files.
    You may use this infrastructure or replace parts of it (or all of it)
    with your own, but your MALH files implementation must conform to
    the conventions used in our code.
    In particular, you should preserve the interfaces to the supplied modules
    (e.g. <tt>Reln</tt>, <tt>Page</tt>, <tt>Query</tt>, <tt>Tuple</tt>) and ensure that
    your submitted ADTs work with the supplied code in the <tt>create</tt>,
    <tt>insert</tt> and <tt>select</tt> commands.
  </p>

  <h2>Setting Up</h2>

  <p>
    You should make a working directory for this assignment and put
    the supplied code there.
    Read the supplied code to make sure that you
    understand all of the data types and operations used in the
    system.
  </p>
  <pre>
$ <b>mkdir <i>your/ass2/directory</i></b>
$ <b>cd <i>your/ass2/directory</i></b>
$ <b>unzip /web/cs9315/19T2/assignments/ass2/ass2.zip</b>
</pre>
  <p>
    You should see the following files in the directory:
  </p>
  <ul>
    <li> <tt>create.c</tt> ... a main program that creates a new MALH relation
    <li> <tt>dump.c</tt> ... a main program that lists all tuples in an MALH relation
    <li> <tt>insert.c</tt> ... a main program that reads tuples and insert them
    <li> <tt>select.c</tt> ... a main program that finds tuples matching a PMR query
    <li> <tt>stats.c</tt> ... a main program that prints info about an MAH relation
    <li> <tt>gendata.c</tt> ... a main program to generate random tuples
    <li> <tt>bits.h</tt>, <tt>bits.c</tt> ... an ADT for bit-strings
    <li> <tt>chvec.h</tt>, <tt>chvec.c</tt> ... an ADT for choice vectors
    <li> <tt>hash.h</tt>, <tt>hash.c</tt> ... the PostgreSQL hash function
    <li> <tt>page.h</tt>, <tt>page.c</tt> ... an ADT for data/overflow pages
    <li> <tt>query.h</tt>, <tt>query.c</tt> ... an ADT for query scanners <span class="green">(incomplete)</span>
    <li> <tt>reln.h</tt>, <tt>reln.c</tt> ... an ADT for relations <span class="green">(partly complete)</span>
    <li> <tt>tuple.h</tt>, <tt>tuple.c</tt> ... an ADT for tuples <span class="green">(partly complete)</span>
    <li> <tt>util.h</tt>, <tt>util.c</tt> ... utility functions
  </ul>
  <p>
    This gives you a partial implementation of MALH files;
    you need to complete the code so that it provides the
    functionality described below.
  </p>
  <p>
    The supplied code actually produces executables that work somewhat,
    but are missing a working query scanner implementation (from <tt>query.c</tt>),
    a proper MA hash function (from <tt>tuple.c</tt>),
    and
    splitting and data file increase (from <tt>reln.c</tt>).
    Effectively, they give a static hash file structure with overflows.
  </p>
  <p>
    To build the executables from the supplied code, do the following:
  </p>
  <pre>
$ <b>make</b>
gcc -Wall -Werror -g -std=c99   -c -o create.o create.c
gcc -Wall -Werror -g -std=c99   -c -o query.o query.c
gcc -Wall -Werror -g -std=c99   -c -o page.o page.c
gcc -Wall -Werror -g -std=c99   -c -o reln.o reln.c
gcc -Wall -Werror -g -std=c99   -c -o tuple.o tuple.c
gcc -Wall -Werror -g -std=c99   -c -o util.o util.c
gcc -Wall -Werror -g -std=c99   -c -o chvec.o chvec.c
gcc -Wall -Werror -g -std=c99   -c -o hash.o hash.c
gcc -Wall -Werror -g -std=c99   -c -o bits.o bits.c
gcc   create.o query.o page.o reln.o tuple.o util.o chvec.o hash.o bits.o   -o create
gcc -Wall -Werror -g -std=c99   -c -o dump.o dump.c
gcc   dump.o query.o page.o reln.o tuple.o util.o chvec.o hash.o bits.o   -o dump
gcc -Wall -Werror -g -std=c99   -c -o insert.o insert.c
gcc   insert.o query.o page.o reln.o tuple.o util.o chvec.o hash.o bits.o   -o insert
gcc -Wall -Werror -g -std=c99   -c -o select.o select.c
gcc   select.o query.o page.o reln.o tuple.o util.o chvec.o hash.o bits.o   -o select
gcc -Wall -Werror -g -std=c99   -c -o stats.o stats.c
gcc   stats.o query.o page.o reln.o tuple.o util.o chvec.o hash.o bits.o   -o stats
gcc -Wall -Werror -g -std=c99   -c -o gendata.o gendata.c
gcc   gendata.o query.o page.o reln.o tuple.o util.o chvec.o hash.o bits.o   -o gendata
</pre>
  <p>
    This should not produce any errors on the CSE servers; let me know ASAP
    if this is not the case.
  </p>
  <p>
    Once you have the executables, you could build a sample database as follows:
  </p>
  <pre>
$ <b>./create  R  3  4  "0,0:0,1:0,2:1,0:1,1:2,0"</b>
cv[0] is (0,0)
cv[1] is (0,1)
cv[2] is (0,2)
cv[3] is (1,0)
cv[4] is (1,1)
cv[5] is (2,0)
cv[6] is (0,31)
cv[7] is (1,31)
cv[8] is (2,31)
...
cv[30] is (0,23)
cv[31] is (1,23)
</pre>
  <p>
    This command creates a new table called <tt>R</tt> with 3 attributes.
    It will be stored in files called <tt>R.info</tt>, <tt>R.data</tt> and <tt>R.ovflow</tt>.
    The data file initially has 4 pages (so depth=2).
    The overflow is initially empty.
    The lower-order 6 bits of the choice vector are given on the command line;
    the remaining bits are auto-generated.
    Given the file size (4 pages), only two of the hash bits are actually needed.
  </p>
  <p>
    You could check the status of the files for table <tt>R</tt>
    via the <tt>stats</tt> command:
    <pre>
$ <b>./stats  R</b>
Global Info:
#attrs:3  #pages:4  #tuples:0  d:2  sp:0
Choice vector
0,0:0,1:0,2:1,0:1,1:2,0:0,31:1,31:2,31:0,30:1,30:2,30:0,29:1,29:2,29:0,28:1,28:2,28:
0,27:1,27:2,27:0,26:1,26:2,26:0,25:1,25:2,25:0,24:1,24:2,24:0,23:1,23
Bucket Info:
#   Info on pages in bucket
    (pageID,#tuples,freebytes,ovflow)
0   (d0,0,1012,-1)
1   (d1,0,1012,-1)
2   (d2,0,1012,-1)
3   (d3,0,1012,-1)
</pre>
    <p>
      Since the file is size 2<sup>d</sup>, the split pointer sp = 0.
      The rest of the global information should be self explanatory,
      as should choice vector.
      The bucket info shows a quadruple for each page; since there are
      no overflow pages (yet), only data pages appear.
      The pageID value in each quad consists of the character <tt>'d'</tt>
      (indicating a data file), plus the page index.
      Each page is 1024 bytes long, which includes a small header, plus
      1012 bytes of free space for tuples.
      There are currently zero tuples in any of the pages.
      The overflow page IDs are all -1 (for <tt>NO_PAGE</tt>) to indicate
      that no data page has an overflow page.
    </p>
    <p>
      You can insert data into the table using the <tt>insert</tt> command
      This command reads tuple from its standard input and inserts them into
      the named table.
      For example, the command below inserts a single tuple into the <tt>R</tt>
      MALH files:
    </p>
    <pre>
$ <b>./insert R</b>
<b>100,abc,xyz</b>
hash(100) = 00011100 00101000 10100111 111011<span class="green">00</span>
<b>Ctl-D</b>
</pre>
    <p>
      The <tt>insert</tt> command prints the hash value for the tuple (based on
      just the first attribute), and then inserts it into the file.
      Since the table is currently empty, this tuple will be inserted into
      page 0.
      Why page 0? You should be able to answer this by knowing the depth
      and the hash value.
      If you then check with the <tt>stats</tt> command you will see that
      there is a single tuple in the files, and it's in page 0.
    </p>
    <p>
      Typing many individual tuples is tedious, so we have provided a
      command, <tt>gendata</tt>, which can generate tuples appropriate
      for a given table. It takes four comand line arguments, only two
      of which are compulsory: the number of tuples to generate, and the
      number of attributes in each tuple. a sample usage:
    </p>
    <pre>
$ $ <b>./gendata  5  3</b>
1,sandwich,pocket
2,circus,spectrum
3,snail,adult
4,crystal,fungus
5,bowl,surveyor
</pre>
    <p>
      This generates five tuples, each with three attributes.
      The first attribute is a unique ID value; the other
      attributes are random words.
      You can modify the starting ID value and the seed for
      the random number generator from the command line.
    </p>
    <p>
      You could use <tt>gendata</tt> to generate large numbers of
      tuples, and insert them as follows:
    </p>
    <pre>
$ <b>./gendata 250 3 101 | ./insert R</b>
hash(101) = 11110100 01100100 11010000 00110000
hash(102) = 00100101 10100110 10100001 11100100
hash(103) = 10110011 11001111 10100111 00001000
hash(104) = 00001100 11100000 10000011 11000000
...
hash(348) = 11110000 01011110 01000010 00101001
hash(349) = 01101101 01100101 00011111 10100111
hash(350) = 10011011 01100101 01111001 11001000
</pre>
    <p>
      This will insert 250 tuples into the table, with ID values
      starting at 101.
      You can check the final state of the database using the <tt>stats</tt>
      command. It should look something like:
      <pre>
$ <b>./stats R</b>
Global Info:
#attrs:3  #pages:4  #tuples:251  d:2  sp:0
Choice vector
0,0:0,1:0,2:1,0:1,1:2,0:0,31:1,31:2,31:0,30:1,30:2,30:0,29:1,29:2,29:0,28:1,28:2,28:
0,27:1,27:2,27:0,26:1,26:2,26:0,25:1,25:2,25:0,24:1,24:2,24:0,23:1,23
Bucket Info:
#    Info on pages in bucket
     (pageID,#tuples,freebytes,ovflow)
[ 0]  (d0,56,4,0) -&gt; (ov0,15,737,-1)
[ 1]  (d1,57,2,3) -&gt; (ov3,2,981,-1)
[ 2]  (d2,59,1,2) -&gt; (ov2,2,976,-1)
[ 3]  (d3,54,7,1) -&gt; (ov1,6,905,-1)
</pre>
      <p>
        This shows that each data page has one overflow page, and that each
        data page has roughly the same number of tuples.
        The bucket starting at data page 0 has a few more tuples than th
        other buckets, because it has more tuples (15) in the overflow page.
        Note that page IDs in the overflow pages are distinguished by starting
        with <tt>"ov"</tt>.
        Note also that e.g. the data page at position 3 in the data file
        has an overflow page at position 1 in the overflow file.
      </p>
      <p>
        One other thing to notice here is that the file has not expanded.
        It still has the 4 original data pages. Even if you added thousands
        of tuples, it would still have only 4 data pages.
        This is because linear hashing is not yet implemented.
        Implementing it is one of your tasks.
        <p>
          You could then use the <tt>select</tt> command to search for tuples
          using a command like:
        </p>
        <pre>
$ <b>./select R 101,?,?</b>
</pre>
        <p>
          This aims to find any tuple with 101 as the ID value; there will be
          one such tuple, since ID values are unique.
          This returns no solutions because query scanning is not yet implemented.
          Implementing it is another of your tasks.
        </p>

        <h2>Task 1: Multi-attribute Hashing</h2>
        <p>
          The current hash function does not use the choice vector to produce
          a combined hash value.
          It simply uses the hash value of the first attribute (the ID value)
          to generate a hash for the tuple.
          Your first task is to modify the <tt>tupleHash()</tt> function to use the
          relevant bits from each attribute hash value to form a composite hash.
          The choice vector determines the "relevant" bits.
          You can find more details on how a multi-attribute hash value is produced
          in the lecture slides and notes.
        </p>

        <h2>Task 2: Selection (Querying)</h2>
        <p>
          The query scan data type is found in <tt>query.c</tt> and <tt>query.h</tt>
          and is used only in <tt>select.c</tt>.
          At present, the data type is incomplete.
          You need to design a suitable query scanning data structure and implement
          the operations on it.
          The functions contain rough approximations to the algorithms you will need
          to build; you can find more details in the lecture slides and course notes.
          Most (all?) of the helper functions you'll need are in other data types,
          but you can add any others that you find necessary.
        </p>

        <h2>Task 3: Linear Hashing</h2>
        <p>
          As noted above, the current implementation is essentially a static
          version of single-attribute hashing.
          You need to add functionality to ensure that the file expands after
          every <i>c</i> insertions, where <i>c</i> is the page capcity
          <i>c = floor(B/R) &approx; 1024/(10*n)</i> where <i>n</i> is
          the number of attributes.
          Add one page at the end of the file and distribute the tuples in the
          "buddy" page (at index 2<sup>d</sup> less) between the old and new
          pages.
          Determine where each tuple goes by considering <i>d+1</i> bits of the
          hash value.
          This will involve modifying the <tt>addToRelation()</tt> function,
          and will most likely require you to add new functions into the
          <tt>reln.c</tt> file (and maybe other files).
        </p>
        <p>
          You can simplify the standard version of linear hashing by <em>not</em>
          removing overflow pages from the overflow chain of the data page
          they are attached to.
          This may result in some data pages having multiple empty overflow
          pages; this is ok if they are eventually used to hold more tuples.
        </p>
        <p>
          The following diagram shows an example of what might occur during
          a page split:
        </p>
        <center><img src="Pics/split.png"></center>

        <h2>How we Test your Submission &nbsp; <span class="red">NEW!</span></h2>
        <p>
          We will compile your submission for testing as follows:
        </p>
        <pre>
$ <b>tar xf <i>YourAss2.tar</i></b>
$ <b>tar xf <i>OurMainPrograms.tar</i></b>
<comment># extracts our copies of ...</comment>
<comment># create.c dump.c insert.c select.c stats.c</comment>
$ <b>make</b>
<comment># should produce executables ...</comment>
<comment># create dump insert select stats</comment>
</pre>
        <p>
          We will then run a range of tests to check that your program
          meets the requirements given above.
        </p>
        <p>
          Since we are using the original <tt>create.c</tt>, etc., your
          code must work with them. The easiest way to ensure this is to
          <em>not</em> change these files while you're working on the assignment.
        </p>

        <h2>Submission</h2>
        <p>
          You need to submit a single <tt>tar</tt> file containing all of
          the code files that are needed to build the <tt>create</tt>,
          <tt>dump</tt>, <tt>insert</tt>, <tt>select</tt> and <tt>stats</tt>
          commands.
        </p>
        <p>
          Note that we will use the original versions of
          <tt>create.c</tt>,
          <tt>dump.c</tt>,
          <tt>insert.c</tt>,
          <tt>select.c</tt>,
          <tt>stats.c</tt>,
          and
          <tt>gendata.c</tt>
          for testing your code.
          This means that any functions you write must use the same
          interface as defined in the ADT <tt>*.h</tt> files.
        </p>
        <p>
          When you want to submit your work, do the following:
        </p>
        <pre>
$ <b>cd <i>your/ass2/directory</i></b>
$ <b>tar cf ass2.tar Makefile *.c *.h</b>
</pre>
        <p>
          Once you have generated the <tt>ass2.tar</tt> file, you can submit
          it via Webcms3 or the <tt>give</tt> command.
        </p>

        <p>Have fun, <i>jas</i></p>
</body>

</html>